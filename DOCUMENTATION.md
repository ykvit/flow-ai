# Flow-AI Backend: Architecture and Design Decisions

This document outlines the architecture of the Flow-AI backend, explaining key design decisions and the overall workflow.

## 1. Core Philosophy

The backend is built on the principles of **Clean Architecture**. This means the code is organized into distinct layers, each with a specific responsibility. The primary benefit is a clear separation of concerns, which makes the system maintainable, testable, and scalable.

## 2. Layers of the Application

- **/cmd/server**: The application's entry point. Its only job is to initialize all components and start the HTTP server. It also contains the root annotations for the OpenAPI specification.
- **/internal/api**: The API Layer. It handles HTTP requests and responses using `go-chi`. Handlers in this layer contain comments-as-code (`swaggo`) that are used to generate the interactive Swagger documentation.
- **/internal/service**: The Business Logic Layer. This is the heart of the application, orchestrating the flow of data.
- **/internal/repository**: The Data Access Layer, responsible for all database interactions with SQLite.
- **/internal/database**: A utility package for initializing the database connection and schema.
- **/internal/llm**: The External Services Layer, abstracting communication with Ollama.
- **/internal/model**: The Domain Layer, defining core data structures (`Chat`, `Message`).
- **/docs**: This directory is **auto-generated by `swag`**. It contains the OpenAPI specification files. Do not edit its contents manually.

**Interactive Documentation:**
The backend serves an interactive Swagger UI. When the server is running, it is accessible at `http://localhost:8000/swagger/index.html`.

## 3. Key Design Decisions

### Database: SQLite over Redis
- **Decision:** The project was migrated from Redis to SQLite.
- **Rationale:** Simplicity, relational power for features like conversation branching, and excellent performance with WAL mode.

### Dynamic Model Configuration
- **Decision:** The application does not rely on hardcoded model names.
- **Rationale:** On first launch, the backend discovers available Ollama models, selects the most recent one as a default, and saves this configuration to the database for persistence.

### Robust Startup Sequence
- **Decision:** The backend actively waits for the Ollama service to be ready before initializing.
- **Rationale:** This makes the application self-reliant and resilient to the unpredictable startup order in containerized environments.

### Structured Title Generation
- **Decision:** The LLM is prompted to return chat titles in a strict JSON format.
- **Rationale:** This provides reliable, structured output. The backend includes "smart parsing" logic to extract this JSON even from noisy model responses.

### Auto-Generated API Documentation
- **Decision:** Use `swaggo/swag` to generate OpenAPI (Swagger) documentation from code comments.
- **Rationale:** This ensures the documentation is always synchronized with the code, provides an interactive UI for testing, and serves as a single source of truth for the API contract.