# Flow-AI Backend: Architecture and Design Decisions

This document outlines the architecture of the Flow-AI backend, explaining key design decisions and the overall workflow.

## 1. Core Philosophy

The backend is built on the principles of **Clean Architecture**. This means the code is organized into distinct layers, each with a specific responsibility. The primary benefit is a clear separation of concerns, which makes the system:
- **Maintainable:** Changes in one layer (e.g., switching the database) have minimal impact on others.
- **Testable:** Each layer can be tested in isolation.
- **Scalable:** Business logic is independent of the web framework and database.

## 2. Layers of the Application

- **/cmd/server**: The application's entry point. Its only job is to initialize all components (config, database, services, router) and start the HTTP server. This is the "glue" that holds everything together.
- **/internal/api**: The API Layer. It handles HTTP requests and responses. It uses `go-chi` for routing. Handlers in this layer are responsible for parsing incoming JSON, calling the appropriate service methods, and formatting the responses. It knows nothing about the database.
- **/internal/service**: The Business Logic Layer. This is the heart of the application. Services orchestrate the flow of data between the repository and external providers (like Ollama). For example, `ChatService` contains the logic for creating messages, generating titles, and managing conversation flow.
- **/internal/repository**: The Data Access Layer. This layer is responsible for all database interactions. It implements a `Repository` interface, which defines all necessary data operations (`CreateChat`, `GetMessages`, etc.). The current implementation uses SQLite (`sqlite_repository.go`), but thanks to the interface, it could be swapped for PostgreSQL with minimal changes to other layers.
- **/internal/database**: A small utility package for initializing the database connection and running schema migrations.
- **/internal/llm**: The External Services Layer. This layer abstracts communication with the Large Language Model. It defines an `LLMProvider` interface, making it possible to switch from Ollama to another provider (like OpenAI or Anthropic) by simply creating a new implementation of the interface.
- **/internal/model**: The Domain Layer. Defines the core data structures (`Chat`, `Message`) used across all layers.

## 3. Key Design Decisions

### Database: SQLite over Redis

- **Decision:** The project was migrated from Redis to SQLite.
- **Rationale:**
    - **Simplicity:** SQLite is an embedded database that requires no separate server, simplifying deployment and local development setup.
    - **Relational Power:** The need to support conversation branching (for message regeneration) and structured data (chats, messages, metadata) is a natural fit for a relational model. Implementing this in Redis would have been complex.
    - **Performance:** With WAL (Write-Ahead Logging) mode enabled, SQLite provides excellent concurrency for web applications with a moderate write load, which is typical for a chat application.
    - **Data Integrity:** Foreign keys and transactions (e.g., `ON DELETE CASCADE`) ensure that the data remains consistent automatically.

### Dynamic Model Configuration

- **Decision:** The application does not rely on hardcoded model names.
- **Rationale:**
    - **Flexibility:** Users can use any models they have downloaded in Ollama.
    - **Robustness:** On first launch, the backend queries Ollama for available models and automatically selects the most recently updated one as the default.
    - **User Control:** Settings are stored in the database. Users can change the default model at any time via the API, and this choice will persist across application restarts.

### Robust Startup Sequence

- **Decision:** The backend actively waits for the Ollama service to be ready before initializing.
- **Rationale:** In a containerized environment (Docker Compose), services can start in an unpredictable order. Instead of relying on complex `depends_on` conditions with health checks, the Go application itself contains a simple loop that polls the Ollama API. This makes the application self-reliant and resilient to slow startup times of its dependencies.

### Structured Title Generation

- **Decision:** When generating chat titles, the LLM is prompted to return a response in a strict JSON format.
- **Rationale:**
    - **Reliability:** Instructing the model to return `{"title": "..."}` is far more reliable than asking for plain text. It minimizes the risk of the model including conversational filler or its own "thoughts" (`<think>...</think>`).
    - **Simplicity:** Parsing a known JSON structure is trivial and eliminates the need for complex string manipulation and regex to "clean up" the model's output.